{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket Lab: \n",
    "## A playground for understanding hydrologic process representations with Deep Learning and Information Theory\n",
    "\n",
    "This notebook is designed to help students understand the fundamentals of simulating dynamic systems with Deep Learning. The data is completely generated synthetically in this notebook, including dynamic forcings representing precipitation and evapotranspiration, representations of the hydrological system (leaking buckets) with a variety of attributes, \"ground truth\" representations of water level and fluxes out of the buckets. This notebook allows the user to experiment with modifications to any aspect of the system. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This cell can be removed after we finish)\n",
    "\n",
    "- Update by Jonathan, May 12th Making the notebook super light, so it will run with Biner.\n",
    "- Updated by Jonathan April 26, 2023\n",
    "    Edited variable names to match the preprint\n",
    "    Edited the evaporation equation. But it is completely artificial, may want to make it more realistic.\n",
    "- Updated by Jonathan April 26, 2023\n",
    "    I am modifying the section titles, and order to match the flow of the preprint\n",
    "    Adding back in the infiltration loss, which I took out for some reason at some point.\n",
    "\n",
    "- Updated by Jonathan\n",
    "    Add option to run with GPU\n",
    "    little tweaks\n",
    "\n",
    "- Update by Leila Hernandez R on April 18, 2023: \n",
    "    I check and ran all cells in the code, and it's working well. \n",
    "    I added a brief explanation in some cells. I added sections 7 and 8. Section 7 increase the number of training buckets, \n",
    "    store the results, and plot loss and RMSE. Section 8 uses Entropy and MI as metrics and needs more work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup notebook\n",
    "\n",
    "The first thing to do is setup the notebook with all the libraries, variables, constants and global parameters declared."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Notebook environment with standard libraries \n",
    "Here we import standard libraries for data management, calculations and plotting. \n",
    "Also, we import machine learning libraries for modeling architectures, training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import machine learning python libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable \n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Global constants, variables and model parameters \n",
    "This cell includes values to define the system we want to represent and the model hyperparameters. \n",
    "In a typical full-scale modeling framework, these would be decalared in a configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "g = 9.807         #[m/s^2]\n",
    "time_step = 1     #[s]\n",
    "\n",
    "# Set the splits for training, validation and testing\n",
    "# Back propogation should be done on the \"train\" set\n",
    "# Hyperparameter tuning should be done on the \"val\" set\n",
    "# Calculating model accuracy should be done on the \"test\" set\n",
    "n_buckets_split = {\"train\":10, \"val\":5,\"test\":1}\n",
    "time_splits = {\"train\":300, \"val\":300,\"test\":200}\n",
    "\n",
    "# Parameters for the forcing processes\n",
    "# These data are created with a simple random process that decides if it is raining and how much, from the previous state. \n",
    "# Three possibilities of precipitation type are:   \n",
    "rain_probability_range = {\n",
    "    \"None\":[0.6, 0.7],       # 1) no precipitation, \n",
    "    \"Light\":[0.5, 0.8],      # 2) light precipitation, and,\n",
    "    \"Heavy\":[0.2, 0.3]       # 3) heavy precipitation.  \n",
    "}\n",
    "# The total ammount of rain is determined by a random uniform distribution.\n",
    "# 1) Light: between zero and P_Light.\n",
    "# 2) Heavy: between P_Light and P_Heavy\n",
    "rain_max = {\"Light\":2, \"Heavy\":8}\n",
    "\n",
    "# Bucket attributes\n",
    "# These set the diversity in the bucket sizes\n",
    "bucket_attributes_range = {\n",
    "    \"A_bucket\":[1.0,1.2], #[m^2]\n",
    "    \"A_spigot\":[0.1,0.12], #[m^2]\n",
    "    \"H_bucket\":[5.0, 5.2], #[m]\n",
    "    \"H_spigot\":[1.0, 1.2], #[m]\n",
    "    \"K_infiltration\": [1e-8, 1e-9],\n",
    "    \"ET_parameter\": [7,8]\n",
    "}\n",
    "bucket_attributes_list = list(bucket_attributes_range.keys())\n",
    "\n",
    "# Add noise to the data, because real world systems are noisy\n",
    "is_noise=True\n",
    "noise = {\"pet\":0.1,    # Noise added to the potential loss to evaporation from the bucket\n",
    "         \"et\":0.1,  # Noise added to the loss to evaporation from the bucket\n",
    "         \"q\":0.1,   # Noise added to the loss of fluxes out from the bucket\n",
    "         \"head\":0.1    # Noise added to the state of the water head in the bucket\n",
    "        }  \n",
    "\n",
    "# LSTM hyperparameters\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")#torch.cuda.device(0)\n",
    "    print(\"Using CUDA device: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device='cpu'\n",
    "hidden_state_size = 16\n",
    "num_layers= 8\n",
    "num_epochs = 5  \n",
    "batch_size = 256 \n",
    "seq_length= 12\n",
    "learning_rate = np.linspace(start=0.1, stop=0.01, num=num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Derived lists and values for setting up the splits in the data\n",
    "Some simple calculations that take the split data from above and creates the values needed to modify the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating splits from the settings above\n",
    "\n",
    "# Bucket splits\n",
    "n_buckets = n_buckets_split[\"train\"] + n_buckets_split[\"val\"] + n_buckets_split[\"test\"]\n",
    "buckets_for_training = list(range(0, n_buckets_split['train']+1))\n",
    "buckets_for_val = list(range(n_buckets_split['train']+1, n_buckets_split['train']+n_buckets_split['val']+1))\n",
    "buckets_for_test = list(range(n_buckets - n_buckets_split['test'], n_buckets))\n",
    "\n",
    "# Time splits\n",
    "train_start = seq_length\n",
    "train_end   = time_splits[\"train\"]\n",
    "val_start   = train_end + seq_length\n",
    "val_end     = val_start + time_splits[\"val\"]\n",
    "test_start  = val_end + seq_length\n",
    "test_end    = test_start + time_splits[\"test\"]\n",
    "\n",
    "# Define buckets (Boundary conditions)\n",
    "buckets = {bucket_attribute:[] for bucket_attribute in bucket_attributes_list}\n",
    "for i in range(n_buckets):\n",
    "    for attribute in bucket_attributes_list:\n",
    "        buckets[attribute].append(np.random.uniform(bucket_attributes_range[attribute][0], \n",
    "                                                    bucket_attributes_range[attribute][1]))\n",
    "        \n",
    "# Initial conditions\n",
    "h_water_level = [np.random.random() for i in range(n_buckets)]\n",
    "mass_overflow = [np.random.random() for i in range(n_buckets)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Variations on the system inputs, creating the synthetic \"precipitation\"\n",
    "These data are created with a simple random process that decides if it is raining, and how much, from the previous state. Three possibilities of precipitation type are  \n",
    "1) no precipitation, 2) light precipitation and 3) heavy precipitation.  \n",
    "\n",
    "The total ammount of rain is determined by a random uniform distribution between zero and P. To simulate a more \"flashy\" system, for instance, reduce the probability of heavy precipitation and increase the magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some synthetic \"precipitation\" as input\n",
    "num_records = time_splits[\"train\"] + time_splits[\"val\"] + time_splits[\"test\"] + seq_length*3\n",
    "in_list = {i:[0] for i in range(n_buckets)}\n",
    "for bucket in range(n_buckets):\n",
    "    \n",
    "    no_rain_probability = np.random.uniform(rain_probability_range[\"None\"][0],\n",
    "                                            rain_probability_range[\"None\"][1])\n",
    "    heavy_rain_probability = np.random.uniform(rain_probability_range[\"Heavy\"][0],\n",
    "                                            rain_probability_range[\"Heavy\"][1])\n",
    "    light_rain_probability = np.random.uniform(rain_probability_range[\"Light\"][0],\n",
    "                                            rain_probability_range[\"Light\"][1])\n",
    "    \n",
    "    for i in range(1,num_records):\n",
    "\n",
    "        # some percent of time we have no rain at all\n",
    "        if np.random.uniform(0.01,0.99) < no_rain_probability:\n",
    "            in_list[bucket].append(0)\n",
    "\n",
    "        # When we do have rain, the probability of heavy or light rain depends on the previous day's rainfall\n",
    "        else:\n",
    "\n",
    "            # If yesterday was a light rainy day, or no rain, then we are likely to have light rain today\n",
    "            # Light rain can come after no rain\n",
    "            if in_list[bucket][i-1] == 0:\n",
    "                if np.random.uniform(0,1) < light_rain_probability:\n",
    "                    in_list[bucket].append(np.random.uniform(0,rain_max[\"Light\"]))\n",
    "                else:\n",
    "                    in_list[bucket].append(0)\n",
    "            else:\n",
    "                # Either light rain or heavy rain\n",
    "                # First give light rain a chance\n",
    "                if np.random.uniform(0.01,0.99) < light_rain_probability:\n",
    "                    in_list[bucket].append(np.random.uniform(0.01,rain_max[\"Light\"]))\n",
    "                # Heavy rain doesn't come out of the blue, it is preceded by at least light rain\n",
    "                elif np.random.uniform(0.01,0.99) < heavy_rain_probability:\n",
    "                    in_list[bucket].append(np.random.uniform(rain_max[\"Light\"],rain_max[\"Heavy\"]))\n",
    "                else:\n",
    "                    in_list[bucket].append(np.random.uniform(0.01,rain_max[\"Light\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Bucket model numerical simulations as \"ground truth\"\n",
    "These represent the data we know about the system, which is what we will try to learn with the LSTM. Even though these are synthetic data, the process representations of a leaking bucket are much simpler than, say, a watershed. So we can be a little more confident that our synthetic leaking buckets represent a real-world system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dictionary = {}\n",
    "\n",
    "for ibuc in range(n_buckets):\n",
    "    \n",
    "    # Memory to store model results\n",
    "    columns = ['precipitaion','et', 'h_bucket', 'q_overflow', 'q_spigot']\n",
    "    columns.extend(bucket_attributes_list)\n",
    "    df = pd.DataFrame(index=list(range(len(in_list[ibuc]))),columns=columns)\n",
    "    \n",
    "    # Main loop through time\n",
    "    for t, precip_in in enumerate(in_list[ibuc]):\n",
    "        \n",
    "        # Add the input mass to the bucket\n",
    "        h_water_level[ibuc] = h_water_level[ibuc] + precip_in\n",
    "\n",
    "        # Lose mass out of the bucket. Some periodic type loss, evaporation, and some infiltration...\n",
    "        et = np.max([0, (buckets[\"A_bucket\"][ibuc] / (buckets[\"ET_parameter\"][ibuc]**2)) * np.sin(t) * np.random.normal(1,noise['pet'])])\n",
    "        infiltration = h_water_level[ibuc] * buckets[\"K_infiltration\"][ibuc]\n",
    "        h_water_level[ibuc] = np.max([0 , (h_water_level[ibuc] - et)])\n",
    "        h_water_level[ibuc] = np.max([0 , (h_water_level[ibuc] - infiltration)])\n",
    "        if is_noise:\n",
    "            h_water_level[ibuc] = h_water_level[ibuc] * np.random.normal(1, noise['et'])\n",
    "\n",
    "        # Overflow if the bucket is too full\n",
    "        if h_water_level[ibuc] > buckets[\"H_bucket\"][ibuc]:\n",
    "            mass_overflow[ibuc] = h_water_level[ibuc] - buckets[\"H_bucket\"][ibuc]\n",
    "            h_water_level[ibuc] = buckets[\"H_bucket\"][ibuc] \n",
    "            if is_noise:\n",
    "                h_water_level[ibuc] = h_water_level[ibuc] - np.random.uniform(0, noise['q'])\n",
    "\n",
    "        # Calculate head on the spigot\n",
    "        h_head_over_spigot = (h_water_level[ibuc] - buckets[\"H_spigot\"][ibuc] ) \n",
    "        if is_noise:\n",
    "            h_head_over_spigot = h_head_over_spigot * np.random.normal(1, noise['head'])\n",
    "\n",
    "        # Calculate water leaving bucket through spigot\n",
    "        if h_head_over_spigot > 0:\n",
    "            velocity_out = np.sqrt(2 * g * h_head_over_spigot)\n",
    "            spigot_out = velocity_out *  buckets[\"A_spigot\"][ibuc] * time_step\n",
    "            h_water_level[ibuc] = h_water_level[ibuc] - spigot_out\n",
    "        else:\n",
    "            spigot_out = 0\n",
    "\n",
    "        # Save the data in time series\n",
    "        df.loc[t,'precipitation'] = precip_in\n",
    "        df.loc[t,'et'] = et\n",
    "        df.loc[t,'h_bucket'] = h_water_level[ibuc]\n",
    "        df.loc[t,'q_overflow'] = mass_overflow[ibuc]\n",
    "        df.loc[t,'q_spigot'] = spigot_out\n",
    "        for attribute in bucket_attributes_list:\n",
    "            df.loc[t,attribute] = buckets[attribute][ibuc]\n",
    "\n",
    "        mass_overflow[ibuc] = 0\n",
    "        \n",
    "    #Now stick that bucket specific data into the general dictionary\n",
    "    bucket_dictionary[ibuc] = df\n",
    "\n",
    "# Set some lists and values based on the input and output variables\n",
    "input_vars = ['precipitation','et']\n",
    "input_vars.extend(bucket_attributes_list)\n",
    "output_vars = ['q_overflow', 'q_spigot']\n",
    "n_input=len(input_vars)\n",
    "n_output=len(output_vars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 Visualize a sample of the bucket fluxes\n",
    "As means to make sure the values are realistic, and that they have both fluxes from the spigot (channel flow) and over the top (flooding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure that the is mass going over the top of the bucket\n",
    "for ibuc in buckets_for_val:\n",
    "    print('Bucket:', ibuc)\n",
    "    print(\"Overflow mean:\", np.round(bucket_dictionary[ibuc].q_overflow.mean(),2))\n",
    "    print(\"Overflow max:\", np.round(bucket_dictionary[ibuc].q_overflow.max(),2))\n",
    "    bucket_dictionary[ibuc].loc[:100,input_vars].plot()\n",
    "    bucket_dictionary[ibuc].loc[:100,output_vars].plot()\n",
    "    bucket_dictionary[ibuc].loc[:100,\"h_bucket\"].plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning model\n",
    "This section sets up our deep learning model and training procedure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7 Define the neural network model\n",
    "This is the part of the notebook that will actually be learning and making the predictions.\n",
    "\n",
    "Brief explanation:\n",
    "The next cell defines a class called LSTM1, which is a PyTorch module for a single-layer Long Short-Term Memory (LSTM) network. \n",
    "The input to the module is a tensor x of shape (batch_size, seq_length, input_size), which represents a sequence of batch_size samples, each of length seq_length, with input_size features at each time step. \n",
    "The LSTM layer is defined using the nn.LSTM class, with input_size as the size of the input layer, hidden_size as the size of the hidden state, and batch_first=True indicating that the first dimension of the input tensor is the batch size. \n",
    "The output of the LSTM layer is passed through a ReLU activation function, and then to a fully connected layer (nn.Linear) with num_classes output units. \n",
    "The forward method takes the input tensor x as an argument, along with an optional tuple init_states representing the initial hidden and internal states of the LSTM layer, and returns the output tensor prediction. \n",
    "If init_states is not provided, it is initialized as a tensor of zeros with shape (batch_size, hidden_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, batch_size, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True) #lstm\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc_1 =  nn.Linear(hidden_size, num_classes) #fully connected 1\n",
    "   \n",
    "    def forward(self, x, init_states=None):\n",
    "\n",
    "        if init_states is None:\n",
    "            h_t = Variable(torch.zeros(batch_size, self.hidden_size)) #hidden state\n",
    "            c_t = Variable(torch.zeros(batch_size, self.hidden_size)) #internal state\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "           \n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.relu(out)\n",
    "        prediction = self.fc_1(out) # Dense, fully connected layer\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.8 Define a procedure for validation\n",
    "This is to make sure that our model is working the way we want it to.  \n",
    "This would be the thing to check when changing hyperparameters.\n",
    "\n",
    "Brief explanation:\n",
    "In this cell we define how to validate and test the LSTM model, as well as check the water balance of the system.\n",
    "We use a pre-defined the LSTM model to make predictions on the validation data. \n",
    "The output of this model is then used to compute two different metrics, the Nash-Sutcliffe Efficiency (NSE) for the spigot_out and mass_overflow columns of the dataframe.\n",
    "Next, the function plots the actual spigot_out and mass_overflow values against their corresponding LSTM predictions. Finally, it checks the water balance of the system by summing up the input, evapotranspiration, mass_overflow, spigot_out, and the last recorded water level in the dataframe, and compares this to the total mass out of or left in the system. It then prints out the percent mass residual as a measure of how well the system is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validation_period(lstm, np_val_seq_X, ibuc, n_plot=100):\n",
    "\n",
    "    \"\"\" Validation procedure\n",
    "    \"\"\"\n",
    "    \n",
    "    df = bucket_dictionary[ibuc]\n",
    "    \n",
    "    lstm_output_val = lstm(torch.Tensor(np_val_seq_X[ibuc]).to(device=device))\n",
    "    \n",
    "    val_spigot_prediction = []\n",
    "    val_overflow_prediction = []\n",
    "    for i in range(lstm_output_val.shape[0]):\n",
    "        val_spigot_prediction.append((lstm_output_val[i,-1,1].cpu().detach().numpy() * \\\n",
    "                                np.std(df.loc[train_start:train_end,'q_spigot'])) + \\\n",
    "                               np.mean(df.loc[train_start:train_end,'q_spigot']))\n",
    "        \n",
    "        val_overflow_prediction.append((lstm_output_val[i,-1,0].cpu().detach().numpy() * \\\n",
    "                                np.std(df.loc[train_start:train_end,'q_overflow'])) + \\\n",
    "                               np.mean(df.loc[train_start:train_end,'q_overflow']))\n",
    "        \n",
    "    spigot_out = df.loc[val_start:val_end, 'q_spigot']\n",
    "    spigot_mean = np.mean(spigot_out)\n",
    "    spigot_pred_variance = 0\n",
    "    spigot_obs_variance = 0\n",
    "\n",
    "    overflow_out = df.loc[val_start:val_end, 'q_overflow']\n",
    "    overflow_mean = np.mean(overflow_out)\n",
    "    overflow_pred_variance = 0\n",
    "    overflow_obs_variance = 0\n",
    "\n",
    "    for i, pred_spigot in enumerate(val_spigot_prediction):\n",
    "        t = i + seq_length - 1\n",
    "        spigot_pred_variance += np.power(( pred_spigot          - spigot_out.values[t]), 2)\n",
    "        spigot_obs_variance  += np.power(( spigot_mean          - spigot_out.values[t]), 2)\n",
    "\n",
    "    for i, pred_overflow in enumerate(val_overflow_prediction):\n",
    "        t = i + seq_length - 1\n",
    "        overflow_pred_variance += np.power((pred_overflow          - overflow_out.values[t]), 2)\n",
    "        overflow_obs_variance  += np.power((overflow_mean          - overflow_out.values[t]), 2)\n",
    "        \n",
    "    print(\"Spigot NSE\",   np.round( 1 - ( spigot_pred_variance   / spigot_obs_variance   ), 4))\n",
    "    print(\"Overflow NSE\", np.round( 1 - ( overflow_pred_variance / overflow_obs_variance ), 4))\n",
    "        \n",
    "    plt.plot(df.loc[val_start+seq_length-1:val_start+n_plot+seq_length-1,'q_spigot'].values, label=\"Spigot out\")\n",
    "    plt.plot(val_spigot_prediction[:n_plot], label=\"LSTM spigot out\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.plot(df.loc[val_start+seq_length-1:val_start+n_plot+seq_length-1,'q_overflow'].values, label=\"Overflow\")\n",
    "    plt.plot(val_overflow_prediction[:n_plot], label=\"LSTM Overflow\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------- #\n",
    "    # Check the water balance of the system\n",
    "    df = bucket_dictionary[ibuc]\n",
    "    \n",
    "    print(\"Mass into the system: \", df.sum()['precipitation'])\n",
    "    mass_out_of_or_left_in_the_bucket = df.sum()['et'] + \\\n",
    "                                        df.sum()['q_overflow']        + \\\n",
    "                                        df.sum()['q_spigot']           + \\\n",
    "                                        df.loc[num_records-1,'h_bucket']\n",
    "    \n",
    "    print(\"Mass out or left over:\", mass_out_of_or_left_in_the_bucket)\n",
    "    print(\"percent mass resudual: {:.0%}\".format((df.sum()['precipitation']-mass_out_of_or_left_in_the_bucket) / \\\n",
    "                                             df.sum()['precipitation']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.9 Instantiate the neural network model\n",
    "\n",
    "Using the hyperparameters from Section 1.2, we define a specific instance of the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LSTM\n",
    "torch.manual_seed(1)\n",
    "lstm = LSTM1(num_classes=n_output,  \n",
    "             input_size=n_input,    \n",
    "             hidden_size=hidden_state_size, \n",
    "             num_layers=num_layers, \n",
    "             batch_size=batch_size, \n",
    "             seq_length=seq_length).to(device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.10 Setup the data, including splits, for training, validation and testing\n",
    "This is what will actually be fed into the model for:\n",
    "1. training, to calculate the loss which is backpropogated through the model\n",
    "2. validationa, where we get predictions from the trained model and see if the performance is up to our standards\n",
    "3. testing, the last thing we would do, and the data we will report. Once the training performance is calculated, cannot go back to validation, otherwise that would be P-hacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all the splits for the model\n",
    "\n",
    "train_loader={}\n",
    "test_loader={} \n",
    "val_loader={}\n",
    "np_train_seq_X={}\n",
    "np_train_seq_y={}\n",
    "np_val_seq_X={}\n",
    "np_val_seq_y={}\n",
    "np_test_seq_X={}\n",
    "np_test_seq_y={}\n",
    "        \n",
    "# Fit all data to the training set\n",
    "frames = [bucket_dictionary[ibuc].loc[train_start:train_end,input_vars] for ibuc in buckets_for_training]\n",
    "df_in = pd.concat(frames)    \n",
    "scaler_in = StandardScaler()\n",
    "scaler_train_in = scaler_in.fit_transform(df_in)\n",
    "\n",
    "frames = [bucket_dictionary[ibuc].loc[train_start:train_end,output_vars] for ibuc in buckets_for_training]\n",
    "df_out = pd.concat(frames)    \n",
    "scaler_out = StandardScaler()\n",
    "scaler_train_out = scaler_out.fit_transform(df_out)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------- #\n",
    "# Training data splits\n",
    "for ibuc in buckets_for_training:\n",
    "\n",
    "    df = bucket_dictionary[ibuc]\n",
    "    \n",
    "    scaler_test_in = scaler_in.transform(df.loc[test_start:test_end,input_vars])\n",
    "    scaler_test_out = scaler_out.transform(df.loc[test_start:test_end,output_vars])\n",
    "\n",
    "    np_train_seq_X[ibuc] = np.zeros((scaler_train_in.shape[0] - seq_length, seq_length, n_input))\n",
    "    np_train_seq_y[ibuc] = np.zeros((scaler_train_out.shape[0] - seq_length, seq_length, n_output))\n",
    "    for i in range(0, scaler_train_in.shape[0] - seq_length):\n",
    "        t = i+seq_length\n",
    "        np_train_seq_X[ibuc][i, :, :] = scaler_train_in[i:t,:]\n",
    "        np_train_seq_y[ibuc][i, :, :] = scaler_train_out[i:t,:]\n",
    "        \n",
    "    ds_train = torch.utils.data.TensorDataset(torch.Tensor(np_train_seq_X[ibuc]), \n",
    "                                              torch.Tensor(np_train_seq_y[ibuc]))\n",
    "    train_loader[ibuc] = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------- #\n",
    "# Validation data splits\n",
    "for ibuc in buckets_for_val:\n",
    "\n",
    "    df = bucket_dictionary[ibuc]\n",
    "       \n",
    "    scaler_val_in = scaler_in.transform(df.loc[val_start:val_end,input_vars])\n",
    "    scaler_val_out = scaler_out.transform(df.loc[val_start:val_end,output_vars])\n",
    "                                                                                           \n",
    "    np_val_seq_X[ibuc] = np.zeros((scaler_val_in.shape[0] - seq_length, seq_length, n_input))\n",
    "    np_val_seq_y[ibuc] = np.zeros((scaler_val_out.shape[0] - seq_length, seq_length, n_output))\n",
    "    for i in range(0, scaler_val_in.shape[0] - seq_length):\n",
    "        t = i+seq_length\n",
    "        np_val_seq_X[ibuc][i, :, :] = scaler_val_in[i:t,:]\n",
    "        np_val_seq_y[ibuc][i, :, :] = scaler_val_out[t,:]\n",
    "\n",
    "        \n",
    "    ds_val = torch.utils.data.TensorDataset(torch.Tensor(np_val_seq_X[ibuc]), \n",
    "                                            torch.Tensor(np_val_seq_y[ibuc]))\n",
    "    val_loader[ibuc] = torch.utils.data.DataLoader(ds_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------- #\n",
    "# Test data splits\n",
    "for ibuc in buckets_for_test:\n",
    "    \n",
    "    df = bucket_dictionary[ibuc]\n",
    "    \n",
    "    scaler_test_in = scaler_in.transform(df.loc[test_start:test_end,input_vars])\n",
    "    scaler_test_out = scaler_out.transform(df.loc[test_start:test_end,output_vars])\n",
    "    \n",
    "    np_test_seq_X[ibuc] = np.zeros((scaler_test_in.shape[0] - seq_length, seq_length, n_input))\n",
    "    np_test_seq_y[ibuc] = np.zeros((scaler_test_out.shape[0] - seq_length, seq_length, n_output))\n",
    "    for i in range(0, scaler_test_in.shape[0] - seq_length):\n",
    "        t = i+seq_length\n",
    "        np_test_seq_X[ibuc][i, :, :] = scaler_test_in[i:t,:]\n",
    "        np_test_seq_y[ibuc][i, :, :] = scaler_test_out[i:t,:]\n",
    "\n",
    "\n",
    "    ds_test = torch.utils.data.TensorDataset(torch.Tensor(np_test_seq_X[ibuc]), \n",
    "                                             torch.Tensor(np_test_seq_y[ibuc]))\n",
    "    test_loader[ibuc] = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=True)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.11 Training the model: Learning the general response of the example dynamic ''hydrologic\" system\n",
    "Now is the time to train the model. Everything above was done in preparation for this step.\n",
    "\n",
    "Here we train the LSTM neural network model with a custom loss function using the Adam optimizer. The training is done for a specified number of epochs and for each epoch, the training data is divided into buckets. For each bucket, the data is loaded using a PyTorch DataLoader and passed through the LSTM model. The output is then compared with the target values using the custom loss function. The gradients are calculated and the optimizer is used to update the weights of the model. We use the tqdm library to show the progress of the training. Finally, we estimate the average RMSE for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # Use costome loss function. \n",
    "optimizer = optim.Adam(lstm.parameters(), lr=learning_rate[0])\n",
    "epoch_bar = tqdm(range(num_epochs),desc=\"Training\", position=0, total=num_epochs)\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    \n",
    "    for ibuc in buckets_for_training:\n",
    "        \n",
    "        batch_bar = tqdm(enumerate(train_loader[ibuc]),\n",
    "                         desc=\"Bucket: {}, Epoch: {}\".format(str(ibuc),str(epoch)),\n",
    "                         position=1,\n",
    "                         total=len(train_loader[ibuc]), leave=False, disable=True)\n",
    "\n",
    "        for i, (data, targets) in batch_bar:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            optimizer = optim.Adam(lstm.parameters(), lr=learning_rate[epoch])\n",
    "\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            # Forward\n",
    "            lstm_output = lstm(data) \n",
    "            loss = criterion(lstm_output,targets)\n",
    "\n",
    "            #backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient descent or adam step\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_bar.set_postfix(loss=loss.to(device).item(),\n",
    "                                  RMSE=\"{:.2f}\".format(loss**(1/2)),\n",
    "                                  epoch=epoch)\n",
    "            batch_bar.update()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rmse_list = []\n",
    "            for i, (data_, targets_) in enumerate(train_loader[ibuc]):\n",
    "                data_ = data_.to(device=device)\n",
    "                targets_ = targets_.to(device=device)\n",
    "                lstm_output_ = lstm(data_)\n",
    "                MSE_ = criterion(lstm_output_, targets_)\n",
    "                rmse_list.append(MSE_**(1/2))\n",
    "\n",
    "        meanrmse = np.mean(np.array(torch.Tensor(rmse_list)))\n",
    "        epoch_bar.set_postfix(loss=loss.cpu().item(),\n",
    "                              RMSE=\"{:.2f}\".format(meanrmse),\n",
    "                              epoch=epoch)\n",
    "        batch_bar.update()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.12 Check that the model works on the validation data\n",
    "Now that we have a trained model, we can see how it works on or validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ibuc in buckets_for_val:\n",
    "    \n",
    "    check_validation_period(lstm, np_val_seq_X, ibuc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Experimentation: training with increasing the number of training buckets \n",
    "Now we will increase the number of training buckets, store the results for each bucket, and plot the comparative results for the loss and RMSE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "criterion = nn.MSELoss()  # Use costome loss function.\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=learning_rate[0])\n",
    "\n",
    "# Increase the number of training buckets\n",
    "buckets_for_training = list(range(1, n_buckets_split[\"train\"]-1))\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "epoch_bar = tqdm(range(num_epochs),desc=\"Training\", position=0, total=num_epochs)\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    \n",
    "    for ibuc in buckets_for_training:\n",
    "        \n",
    "        batch_bar = tqdm(enumerate(train_loader[ibuc]),\n",
    "                         desc=\"Bucket: {}, Epoch: {}\".format(str(ibuc),str(epoch)),\n",
    "                         position=1,\n",
    "                         total=len(train_loader[ibuc]), leave=False, disable=True)\n",
    "\n",
    "        for i, (data, targets) in batch_bar:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            optimizer = optim.Adam(lstm.parameters(), lr=learning_rate[epoch])\n",
    "\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            # Forward\n",
    "            lstm_output = lstm(data) \n",
    "            loss = criterion(lstm_output,targets)\n",
    "\n",
    "            #backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient descent or adam step\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_bar.set_postfix(loss=loss.cpu().item(),\n",
    "                                  RMSE=\"{:.2f}\".format(loss**(1/2)),\n",
    "                                  epoch=epoch)\n",
    "            batch_bar.update()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rmse_list = []\n",
    "            for i, (data_, targets_) in enumerate(train_loader[ibuc]):\n",
    "                data_ = data_.to(device=device)\n",
    "                targets_ = targets_.to(device=device)\n",
    "                lstm_output_ = lstm(data_)\n",
    "                MSE_ = criterion(lstm_output_, targets_)\n",
    "                rmse_list.append(MSE_**(1/2))\n",
    "\n",
    "        meanrmse = np.mean(np.array(torch.Tensor(rmse_list)))\n",
    "        epoch_bar.set_postfix(loss=loss.cpu().item(),\n",
    "                              RMSE=\"{:.2f}\".format(meanrmse),\n",
    "                              epoch=epoch)\n",
    "        batch_bar.update()\n",
    "\n",
    "        # Store the results for each bucket\n",
    "        if ibuc not in results:\n",
    "            results[ibuc] = {\"loss\": [], \"RMSE\": []}\n",
    "        results[ibuc][\"loss\"].append(loss.cpu().item())\n",
    "        results[ibuc][\"RMSE\"].append(meanrmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "criterion = nn.MSELoss()  # Use costome loss function.\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=learning_rate[0])\n",
    "\n",
    "# Increase the number of training buckets\n",
    "buckets_for_training = list(range(1, 5))\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "epoch_bar = tqdm(range(num_epochs),desc=\"Training\", position=0, total=num_epochs)\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    \n",
    "    for ibuc in buckets_for_training:\n",
    "        \n",
    "        batch_bar = tqdm(enumerate(train_loader[ibuc]),\n",
    "                         desc=\"Bucket: {}, Epoch: {}\".format(str(ibuc),str(epoch)),\n",
    "                         position=1,\n",
    "                         total=len(train_loader[ibuc]), leave=False, disable=True)\n",
    "\n",
    "        for i, (data, targets) in batch_bar:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            optimizer = optim.Adam(lstm.parameters(), lr=learning_rate[epoch])\n",
    "\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            # Forward\n",
    "            lstm_output = lstm(data) \n",
    "            loss = criterion(lstm_output,targets)\n",
    "\n",
    "            #backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient descent or adam step\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_bar.set_postfix(loss=loss.cpu().item(),\n",
    "                                  RMSE=\"{:.2f}\".format(loss**(1/2)),\n",
    "                                  epoch=epoch)\n",
    "            batch_bar.update()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rmse_list = []\n",
    "            for i, (data_, targets_) in enumerate(train_loader[ibuc]):\n",
    "                data_ = data_.to(device=device)\n",
    "                targets_ = targets_.to(device=device)\n",
    "                lstm_output_ = lstm(data_)\n",
    "                MSE_ = criterion(lstm_output_, targets_)\n",
    "                rmse_list.append(MSE_**(1/2))\n",
    "\n",
    "        meanrmse = np.mean(np.array(torch.Tensor(rmse_list)))\n",
    "        epoch_bar.set_postfix(loss=loss.cpu().item(),\n",
    "                              RMSE=\"{:.2f}\".format(meanrmse),\n",
    "                              epoch=epoch)\n",
    "        batch_bar.update()\n",
    "\n",
    "        # Store the results for each bucket\n",
    "        if ibuc not in results:\n",
    "            results[ibuc] = {\"loss\": [], \"RMSE\": []}\n",
    "        results[ibuc][\"loss\"].append(loss.cpu().item())\n",
    "        results[ibuc][\"RMSE\"].append(meanrmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparative results\n",
    "for ibuc in results:\n",
    "    plt.plot(results[ibuc][\"loss\"], label=\"Bucket {}\".format(ibuc))\n",
    "plt.title(\"Loss vs Epoch for Different Buckets\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for ibuc in results:\n",
    "    plt.plot(results[ibuc][\"RMSE\"], label=\"Bucket {}\".format(ibuc))\n",
    "plt.title(\"RMSE vs Epoch for Different Buckets\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Experimentation: Evaluating the number of training buckets using Information Theory metrics\n",
    "Now we use Entropy and Mutual Information as metrics of evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from scipy.stats import entropy, mutual_info_score\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "criterion = nn.MSELoss()  # Use costome loss function.\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=learning_rate[0])\n",
    "\n",
    "# Increase the number of training buckets\n",
    "buckets_for_training = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "epoch_bar = tqdm(range(num_epochs),desc=\"Training\", position=0, total=num_epochs)\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    \n",
    "    for ibuc in buckets_for_training:\n",
    "        \n",
    "        batch_bar = tqdm(enumerate(train_loader[ibuc]),\n",
    "                         desc=\"Bucket: {}, Epoch: {}\".format(str(ibuc),str(epoch)),\n",
    "                         position=1,\n",
    "                         total=len(train_loader[ibuc]))\n",
    "\n",
    "        for i, (data, targets) in batch_bar:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            optimizer = optim.Adam(lstm.parameters(), lr=learning_rate[epoch])\n",
    "\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            # Forward\n",
    "            lstm_output = lstm(data) \n",
    "            loss = criterion(lstm_output,targets)\n",
    "\n",
    "            #backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient descent or adam step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute entropy and mutual information\n",
    "            entropy_val = entropy(lstm_output.cpu().detach().numpy().flatten())\n",
    "            mi_val = mutual_info_score(targets.cpu().detach().numpy().flatten(), lstm_output.cpu().detach().numpy().flatten())\n",
    "\n",
    "            batch_bar.set_postfix(loss=loss.cpu().item(),\n",
    "                                  RMSE=\"{:.2f}\".format(loss**(1/2)),\n",
    "                                  entropy=\"{:.2f}\".format(entropy_val),\n",
    "                                  MI=\"{:.2f}\".format(mi_val),\n",
    "                                  epoch=epoch)\n",
    "            batch_bar.update()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rmse_list = []\n",
    "            entropy_list = []\n",
    "            mi_list = []\n",
    "            for i, (data_, targets_) in enumerate(train_loader[ibuc]):\n",
    "                data_ = data_.to(device=device)\n",
    "                targets_ = targets_.to(device=device)\n",
    "                lstm_output_ = lstm(data_)\n",
    "                MSE_ = criterion(lstm_output_, targets_)\n",
    "                rmse_list.append(MSE_**(1/2))\n",
    "                entropy_val_ = entropy(lstm_output_.cpu().detach().numpy().flatten())\n",
    "                mi_val_ = mutual_info_score(targets_.cpu().detach().numpy().flatten(), lstm_output_.cpu().detach().numpy().flatten())\n",
    "                entropy_list.append(entropy_val_)\n",
    "                mi_list.append(mi_val_)\n",
    "\n",
    "        epoch_bar.set_postfix(loss=loss.cpu().item(),\n",
    "                              RMSE=\"{:.2f}\".format(np.mean(np.array(rmse_list))),\n",
    "                              entropy=\"{:.2f}\".format(np.mean(np.array(entropy_list))),\n",
    "                              MI=\"{:.2f}\".format(np.mean(np.array(mi_list))),\n",
    "                              epoch=epoch)\n",
    "        batch_bar.update()\n",
    "\n",
    "        # Store the results for each bucket\n",
    "        if ibuc not in results:\n",
    "            results[ibuc] = {\"loss\": [], \"RMSE\": [], \"entropy\": [], \"MI\": []}\n",
    "        results[ibuc][\"loss\"].append(loss.cpu().item())\n",
    "        results[ibuc][\"RMSE\"].append(np.mean(np.array(rmse_list)))\n",
    "        results[ibuc][\"entropy\"].append(np.mean(np.array(entropy_list)))\n",
    "        results[ibuc][\"MI\"].append(np.mean(np.array(mi_list)))\n",
    "\n",
    "# Plot the comparative results, first plot entropy \n",
    "for ibuc in results:\n",
    "    plt.plot(results[ibuc][\"entropy\"], label=\"Bucket {}\".format(ibuc))\n",
    "plt.title(\"Entropy vs Epoch for Different Buckets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
